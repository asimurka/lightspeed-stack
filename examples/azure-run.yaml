apis:
  - agents
  - batches
  - datasetio
  - eval
  - files
  - inference
  - safety
  - scoring
  - tool_runtime
  - vector_io
benchmarks: []
conversations_store:
  db_path: ~/.llama/storage/conversations.db
  type: sqlite
datasets: []
image_name: starter
inference_store:
  db_path: ~/.llama/storage/inference-store.db
  type: sqlite
metadata_store:
  db_path: ~/.llama/storage/registry.db
  type: sqlite
providers:
  agents:
    - config:
        persistence:
          agent_state:
            backend: kv_default
            namespace: agents_state
          responses:
            backend: sql_default
            table_name: agents_responses
      provider_id: meta-reference
      provider_type: inline::meta-reference
  batches:
    - config:
        kvstore:
          backend: kv_default
          namespace: batches_store
      provider_id: reference
      provider_type: inline::reference
  datasetio:
    - config:
        kvstore:
          backend: kv_default
          namespace: huggingface_datasetio
      provider_id: huggingface
      provider_type: remote::huggingface
    - config:
        kvstore:
          backend: kv_default
          namespace: localfs_datasetio
      provider_id: localfs
      provider_type: inline::localfs
  eval:
    - config:
        kvstore:
          backend: kv_default
          namespace: eval_store
      provider_id: meta-reference
      provider_type: inline::meta-reference
  files:
    - config:
        metadata_store:
          backend: sql_default
          table_name: files_metadata
        storage_dir: ~/.llama/storage
      provider_id: meta-reference-files
      provider_type: inline::localfs
  inference:
    - config:
        api_base: https://ols-test.openai.azure.com/
        api_key: ${env.AZURE_API_KEY}
        api_version: 2025-01-01-preview
      provider_id: azure
      provider_type: remote::azure
  safety:
    - config:
        excluded_categories: []
      provider_id: llama-guard
      provider_type: inline::llama-guard
  scoring:
    - config: {}
      provider_id: basic
      provider_type: inline::basic
    - config: {}
      provider_id: llm-as-judge
      provider_type: inline::llm-as-judge
  tool_runtime:
    - config: {}
      provider_id: rag-runtime
      provider_type: inline::rag-runtime
  vector_io:
    - config:
        persistence:
          backend: kv_default
          namespace: faiss_store
      provider_id: faiss
      provider_type: inline::faiss
registered_resources:
  benchmarks: []
  datasets: []
  models: []
  scoring_fns: []
  shields: []
  tool_groups:
    - provider_id: rag-runtime
      toolgroup_id: builtin::rag
  vector_dbs: []
scoring_fns: []
server:
  port: 8321
storage:
  backends:
    kv_default:
      db_path: ~/.llama/storage/kv_store.db
      type: kv_sqlite
    sql_default:
      db_path: ~/.llama/storage/sql_store.db
      type: sql_sqlite
  stores:
    conversations:
      backend: sql_default
      table_name: openai_conversations
    inference:
      backend: sql_default
      max_write_queue_size: 10000
      num_writers: 4
      table_name: inference_store
    metadata:
      backend: kv_default
      namespace: registry
    prompts:
      backend: kv_default
      namespace: prompts
version: 2
